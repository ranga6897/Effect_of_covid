{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "# pd.set_option('display.max_rows',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_before = pd.read_excel(\"capstone_final_addedrows.xlsx\", sheet_name=2)\n",
    "data_present = pd.read_excel('capstone_final_addedrows.xlsx', sheet_name = 3)\n",
    "descrption = pd.read_excel('capstone_final_addedrows.xlsx', sheet_name=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate column\n",
    "data_present[['Emp_stat_Present','Emp_stat_Present.1']].head(50);\n",
    "\n",
    "data_present.drop(columns='Emp_stat_Present.1',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows , which have less than 40 values (out of 45)   # add_main\n",
    "# removing 17 rows out of 551.\n",
    "\n",
    "data_present_ = data_present.dropna(thresh = 40)\n",
    "data_before_ = data_before.iloc[data_present_.index,: ]\n",
    "\n",
    "data_present_.reset_index(inplace = True)\n",
    "data_before_.reset_index(inplace = True)\n",
    "\n",
    "data_present_.drop(columns = ['index'],inplace = True)\n",
    "data_before_.drop(columns = ['index'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((551, 43), (551, 45))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_before.shape , data_present.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((534, 43), (534, 45))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_before_.shape , data_present_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RangeIndex(start=0, stop=534, step=1), RangeIndex(start=0, stop=534, step=1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_before_.index , data_present_.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check we have same IDs in before and Present datasets.\n",
    "\n",
    "check = []\n",
    "for i,j in zip(data_present.ID,data_before.ID):\n",
    "    check.append(i - j)\n",
    "    \n",
    "sum(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_present_.Age.fillna('25-30' , inplace= True)   # for Id's 315,373 filling age group as 25-30 add_main\n",
    "data_before_.Age.fillna('25-30' , inplace= True)   # for Id's 315,373 filling age group as 25-30 add_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_present_.loc[400,'Gender'] = 'Male' # replacing Prefer_not_to_say to Male : smoking,drinking      #add_main\n",
    "data_present_.loc[401,'Gender'] = 'Male'  # replacing Prefer_not_to_say to Male : no track of household\n",
    "data_present_.loc[[210,297,399],'Gender'] = 'Female' #replacing prefer_not_to_say to Female, keeps track of house_hold\n",
    "data_present_.loc[277,'Gender'] = 'Female'\n",
    "\n",
    "data_before_.loc[400,'Gender'] = 'Male' # replacing Prefer_not_to_say to Male : smoking,drinking      #add_main\n",
    "data_before_.loc[401,'Gender'] = 'Male'  # replacing Prefer_not_to_say to Male : no track of household\n",
    "data_before_.loc[[210,297,399],'Gender'] = 'Female' #replacing prefer_not_to_say to Female, keeps track of house_hold\n",
    "data_before_.loc[277,'Gender'] = 'Female'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_present_.loc[[159,310],'Marital_status'] = 'No'  # student & age : 20-25, id:110(iloc=107) --> age : 25-30  #add_main\n",
    "data_present_.loc[[357,383],'Marital_status'] = 'Yes' # age : 40-55\n",
    "\n",
    "data_before_.loc[[159,310],'Marital_status'] = 'No'  # student & age : 20-25, id:110(iloc=107) --> age : 25-30  #add_main\n",
    "data_before_.loc[[357,383],'Marital_status'] = 'Yes' # age : 40-55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_present_.loc[[58,61,237,291],'Income'] = \"I don't earn right now (maybe in future)\" # student #add_main\n",
    "data_present_.loc[[91,149,154,370],'Income'] = '3 - 5 lakh per annum' #  age --> 20-25 & not married & private sector employee\n",
    "data_present_.loc[[142,165,404,357],'Income'] = '10 - 20 lakh per annum' #  age --> 40-55  & private sector employee & married\n",
    "data_present_.loc[[320,460,488,287], 'Income'] = '5 - 10 lakh per annum' # age --> 25- 30 & public_sector | married male |married Female\n",
    "\n",
    "data_before_.loc[[58,61,237,291],'Income'] = \"I don't earn right now (maybe in future)\" # student #add_main\n",
    "data_before_.loc[[91,149,154,370],'Income'] = '3 - 5 lakh per annum' #  age --> 20-25 & not married & private sector employee\n",
    "data_before_.loc[[142,165,404,357],'Income'] = '10 - 20 lakh per annum' #  age --> 40-55  & private sector employee & married\n",
    "data_before_.loc[[320,460,488,287], 'Income'] = '5 - 10 lakh per annum' # age --> 25- 30 & public_sector| married male | married female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_present_['Loc'].replace(\n",
    "    {\n",
    "        'Telangana': 'south_india',\n",
    "        'Mars': 'south_india',\n",
    "        'Karnataka': 'south_india',\n",
    "        'paris': 'foreign',\n",
    "        'Andhra Pradesh':'south_india',  \n",
    "        'Punjab': 'north_india',\n",
    "        'Chandigarh': 'north_india',\n",
    "        'Jharkhand': 'north_india',\n",
    "        'USA': 'foreign',\n",
    "        'Madhya Pradesh': 'north_india',\n",
    "        'West Bengal': 'north_india',\n",
    "        'Chattisgarh': 'north_india',\n",
    "        'Maharashtra': 'north_india',\n",
    "        'Uttar Pradesh': 'north_india',\n",
    "        'Odisha': 'north_india',\n",
    "        'Haryana': 'north_india',\n",
    "        'Delhi': 'north_india',\n",
    "        'Tamil nadu': 'south_india',\n",
    "        'Hertfordshire': 'foreign',\n",
    "        'sydney': 'foreign',\n",
    "        'Melbourne': 'foreign',\n",
    "        'Gujarat ': 'north_india',\n",
    "        'Rajasthan': 'north_india',\n",
    "        'Bihar': 'north_india',\n",
    "        'Texas': 'foreign',\n",
    "        'Pennsylvania': 'foreign',\n",
    "        'singapore': 'foreign',\n",
    "        'Sikkim': 'north_india',\n",
    "        'Up':'north_india' , \n",
    "        'Shahdol' : 'north_india',\n",
    "       'Mp':'north_india',\n",
    "        'West Bengal ': 'north_india',\n",
    "        'Hyderabad':'south_india',\n",
    "        'Himachal Pradesh' :'south_india',\n",
    "        'TELANGANA' : 'south_india',\n",
    "       'Telengana':'south_india', \n",
    "        'Madhupur (Telangana)' : 'south_india',\n",
    "        'HYD' :'south_india',\n",
    "        'Andhra pradesh' :'south_india',\n",
    "       'Telangana ' :'south_india',\n",
    "        'Maharashtra ':'north_india',\n",
    "        'Dubai' :'foreign',\n",
    "        'Puppalaguda' :'south_india',\n",
    "       'Andhra Pradesh ':'south_india',\n",
    "        'UP' : 'north_india',\n",
    "        'visakhapatnam': 'south_india', \n",
    "        'Vishakapatnam' :'south_india',\n",
    "       'Bhopal': 'north_india',\n",
    "        'US': 'foreign',\n",
    "        'Chhattisgarh':'north_india' ,\n",
    "        'Pune': 'north_india',\n",
    "        'Bangalore': 'south_india',\n",
    "        'Bhilai' :'north_india',\n",
    "       'Tamilnadu':'south_india',\n",
    "        'CG':'north_india',\n",
    "        'New delhi' : 'north_india',\n",
    "        'Chennai':'south_india',\n",
    "        'Tamil Nadu' : 'south_india'\n",
    "        \n",
    "    },\n",
    "    inplace=True)\n",
    "\n",
    "data_before_['Loc'].replace(\n",
    "    {\n",
    "        'Telangana': 'south_india',\n",
    "        'Mars': 'south_india',\n",
    "        'Karnataka': 'south_india',\n",
    "        'paris': 'foreign',\n",
    "        'Andhra Pradesh':'south_india',  \n",
    "        'Punjab': 'north_india',\n",
    "        'Chandigarh': 'north_india',\n",
    "        'Jharkhand': 'north_india',\n",
    "        'USA': 'foreign',\n",
    "        'Madhya Pradesh': 'north_india',\n",
    "        'West Bengal': 'north_india',\n",
    "        'Chattisgarh': 'north_india',\n",
    "        'Maharashtra': 'north_india',\n",
    "        'Uttar Pradesh': 'north_india',\n",
    "        'Odisha': 'north_india',\n",
    "        'Haryana': 'north_india',\n",
    "        'Delhi': 'north_india',\n",
    "        'Tamil nadu': 'south_india',\n",
    "        'Hertfordshire': 'foreign',\n",
    "        'sydney': 'foreign',\n",
    "        'Melbourne': 'foreign',\n",
    "        'Gujarat ': 'north_india',\n",
    "        'Rajasthan': 'north_india',\n",
    "        'Bihar': 'north_india',\n",
    "        'Texas': 'foreign',\n",
    "        'Pennsylvania': 'foreign',\n",
    "        'singapore': 'foreign',\n",
    "        'Sikkim': 'north_india',\n",
    "        'Up':'north_india' , \n",
    "        'Shahdol' : 'north_india',\n",
    "       'Mp':'north_india',\n",
    "        'West Bengal ': 'north_india',\n",
    "        'Hyderabad':'south_india',\n",
    "        'Himachal Pradesh' :'south_india',\n",
    "        'TELANGANA' : 'south_india',\n",
    "       'Telengana':'south_india', \n",
    "        'Madhupur (Telangana)' : 'south_india',\n",
    "        'HYD' :'south_india',\n",
    "        'Andhra pradesh' :'south_india',\n",
    "       'Telangana ' :'south_india',\n",
    "        'Maharashtra ':'north_india',\n",
    "        'Dubai' :'foreign',\n",
    "        'Puppalaguda' :'south_india',\n",
    "       'Andhra Pradesh ':'south_india',\n",
    "        'UP' : 'north_india',\n",
    "        'visakhapatnam': 'south_india', \n",
    "        'Vishakapatnam' :'south_india',\n",
    "       'Bhopal': 'north_india',\n",
    "        'US': 'foreign',\n",
    "        'Chhattisgarh':'north_india' ,\n",
    "        'Pune': 'north_india',\n",
    "        'Bangalore': 'south_india',\n",
    "        'Bhilai' :'north_india',\n",
    "       'Tamilnadu':'south_india',\n",
    "        'CG':'north_india',\n",
    "        'New delhi' : 'north_india',\n",
    "        'Chennai':'south_india',\n",
    "        'Tamil Nadu' : 'south_india'\n",
    "        \n",
    "    },\n",
    "    inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_present_['Spending_kind_Present'].replace({'Medium spending':'medium spending','Low spending':'low spending','High spending':'high spending'},inplace = True)\n",
    "data_before_['Spending_kind_Before'].replace({'Medium spending':'medium spending','Low spending':'low spending','High spending':'high spending'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoder cannot take NaNs, so we shld drop nans but preserve the index.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data_before_copy = data_before_.copy(deep = True)\n",
    "df_before_labels = pd.DataFrame()\n",
    "for col in data_before_.select_dtypes('object').columns:\n",
    "    \n",
    "    data_dropped = data_before_[col].dropna()\n",
    "    not_null_values = data_dropped.values\n",
    "    not_null_index = data_dropped.index\n",
    "    \n",
    "    null_index = data_before_[col].drop(index=not_null_index).index\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    labeled_data = le.fit_transform(not_null_values)\n",
    "      \n",
    "#     print(col,dict(list(enumerate(le.classes_))))  #Un comment to see labels.\n",
    "\n",
    "#   To store the labels and classes\n",
    "    df_dummi = pd.DataFrame()\n",
    "    df_dummi[col] = list(enumerate(le.classes_))\n",
    "    \n",
    "    df_before_labels = pd.concat([df_before_labels,df_dummi],ignore_index=True)\n",
    "    \n",
    "    series_1 = pd.Series(data = labeled_data, index = not_null_index)\n",
    "    series_2 = pd.Series(data = np.NaN, index = null_index)\n",
    "    \n",
    "    data_before_copy[col] = pd.concat([series_1, series_2]).sort_index()\n",
    "    \n",
    "#     print('---'*30)\n",
    "#     print('')\n",
    "\n",
    "#     break\n",
    "\n",
    "\n",
    "\n",
    "data_present_copy = data_present_.copy(deep = True)\n",
    "df_present_labels = pd.DataFrame()\n",
    "for col in data_present_.select_dtypes('object').columns:\n",
    "    \n",
    "    data_dropped = data_present_[col].dropna()\n",
    "    not_null_values = data_dropped.values\n",
    "    not_null_index = data_dropped.index\n",
    "    \n",
    "    null_index = data_present_[col].drop(index=not_null_index).index\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    labeled_data = le.fit_transform(not_null_values)\n",
    "      \n",
    "#     print(col,dict(list(enumerate(le.classes_)))) #Un comment to see labels\n",
    "\n",
    "#   To store the labels and classes\n",
    "    df_dummi = pd.DataFrame()\n",
    "    df_dummi[col] = list(enumerate(le.classes_))\n",
    "    \n",
    "    df_present_labels = pd.concat([df_present_labels,df_dummi],ignore_index=True)\n",
    "    \n",
    "    series_1 = pd.Series(data = labeled_data, index = not_null_index)\n",
    "    series_2 = pd.Series(data = np.NaN, index = null_index)\n",
    "    \n",
    "    data_present_copy[col] = pd.concat([series_1, series_2]).sort_index()\n",
    "    \n",
    "#     print('---'*30)\n",
    "#     print('')\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5 - 10 lakh per annum)                       1\n",
       "(2, 3 - 5 lakh per annum)                        1\n",
       "(5, Under 3 lakh per annum)                      1\n",
       "(1, 20+ lakh per annum)                          1\n",
       "(4, I don't earn right now (maybe in future))    1\n",
       "(0, 10 - 20 lakh per annum)                      1\n",
       "Name: Income, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_present_labels['Income'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5 - 10 lakh per annum)                       1\n",
       "(2, 3 - 5 lakh per annum)                        1\n",
       "(1, 20+ lakh per annum)                          1\n",
       "(4, I don't earn right now (maybe in future))    1\n",
       "(5, Under 3 lakh per annum)                      1\n",
       "(0, 10 - 20 lakh per annum)                      1\n",
       "Name: Income, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_before_labels['Income'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show that above code is working properly\n",
    "data_present_['Age'].value_counts(dropna = False),data_present_copy['Age'].value_counts(dropna= False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_present_copy.drop(columns='ID',inplace = True)\n",
    "data_before_copy.drop(columns='ID',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(data_present_copy, col):      \n",
    "    column_x = data_present_copy.drop(columns=col)\n",
    "    column_y = data_present_copy[col]\n",
    "\n",
    "    column_y_not_null = column_y.dropna()\n",
    "    column_x_y_not_null = column_x.loc[column_y_not_null.index.values]\n",
    "\n",
    "    column_x_not_null = column_x_y_not_null.dropna()\n",
    "    column_y_x_not_null = column_y_not_null.loc[column_x_not_null.index.values]\n",
    "\n",
    "    \n",
    "    column_y_predict = column_y.drop(index= column_y_not_null.index.values)\n",
    "    column_y_predict_index = column_y_predict.index\n",
    "    \n",
    "    column_x_predict = column_x.loc[column_y_predict_index]\n",
    "    \n",
    "    return column_x_not_null,column_y_x_not_null,column_y_predict,column_y_predict_index,column_x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age : mean f1 score          0.45647397017673913\n",
      "Income : mean f1 score          0.4124803846564598\n",
      "Emp_stat_Present : mean f1 score          0.5979988241490546\n",
      "Gender : mean f1 score          0.5780569869554638\n",
      "Marital_status : mean f1 score          0.8999594207990619\n",
      "Loc : mean f1 score          0.5683028179216091\n",
      "Notice_things_Present : mean f1 score          0.6104056447345092\n",
      "All_angles_Present : mean f1 score          0.5697484070259371\n",
      "Sincere_prod_Present : mean f1 score          0.4719968600117654\n",
      "Lonely_Present : mean f1 score          0.2774175994674997\n",
      "Worry_health_Present : mean f1 score          0.4190233817103321\n",
      "Charity_Present : mean f1 score          0.3286470241112937\n",
      "New_env_Present : mean f1 score          0.3424800289037918\n",
      "Meeting_ppl_Present : mean f1 score          0.29739475688691713\n",
      "Hob_interests_Present : mean f1 score          0.378609477125164\n",
      "Surveys_Present : mean f1 score          0.34307664593978315\n",
      "Spent_onli_Present : mean f1 score          0.5401252717860433\n",
      "WFH_office_Present : mean f1 score          0.5056057795572999\n",
      "Ethu_Present : mean f1 score          0.3459237635446557\n",
      "Income_Change : mean f1 score          0.5577150489360155\n",
      "Smoking_hab_Present : mean f1 score          0.7422594919502205\n",
      "Drinking_hab_Present : mean f1 score          0.6966689899353173\n",
      "Sleeping_hab_Present : mean f1 score          0.3734741105226444\n",
      "Healthy_Lifestyle_Present : mean f1 score          0.3425062392097173\n",
      "Medi_pref_Present : mean f1 score          0.5298020264325252\n",
      "Pol_interest_Present : mean f1 score          0.335274703782412\n",
      "Internet_interest_Present : mean f1 score          0.3961504232504521\n",
      "Economy_Manag_intrst_Present : mean f1 score          0.44847534415504287\n",
      "Medicine_intrst_Present : mean f1 score          0.4012453163582899\n",
      "Religion_intrst_Present : mean f1 score          0.30540208869044355\n",
      "Save_all_money_Present : mean f1 score          0.3488764987049053\n",
      "Brand_non-brand_Present : mean f1 score          0.4415045973883239\n",
      "Food_pref_Present : mean f1 score          0.8622889752535705\n",
      "Mode_of_transport_Present : mean f1 score          0.49596381167030074\n",
      "Basic_medications_Present : mean f1 score          0.6581971743733547\n",
      "Digital_content_Present : mean f1 score          0.4720728453283404\n",
      "Active_Investor_Present : mean f1 score          0.21082645827048402\n",
      "Edu_instit_fee_Present : mean f1 score          0.39338158526382366\n",
      "Change_in_data_consumption : mean f1 score          0.5663150938815344\n",
      "Spend_on_gadgets_Present : mean f1 score          0.3269953588386994\n",
      "Spend_on_Luxury_Present : mean f1 score          0.4237344031224442\n",
      "Track_Household_exp_Present : mean f1 score          0.6199626443609174\n",
      "Domestic_help_Present : mean f1 score          0.35616713962779756\n",
      "Spending_kind_Present : mean f1 score          0.5060214003525334\n",
      "\n",
      "mean of all scores: 0.4717047457982612 ,max: 0.8999594207990619, min: 0.21082645827048402\n",
      "Wall time: 2.85 s\n"
     ]
    }
   ],
   "source": [
    "# nulls = data_present_copy.isnull().sum().values\n",
    "# scores_all = []\n",
    "\n",
    "# for col,null in zip(data_present_copy.columns,nulls):\n",
    "    \n",
    "#     dt = DecisionTreeClassifier(random_state=0,max_depth=4)\n",
    "    \n",
    "#     med_x_not_null,med_y_x_not_null,_,_,_ = sort_data(data_present_copy,col)\n",
    "#     scores=cross_val_score(dt,med_x_not_null,med_y_x_not_null,cv=5,scoring='f1_weighted')\n",
    "    \n",
    "#     scores_all.append(np.mean(scores))\n",
    "#     print(f'{col} : mean f1 score          {np.mean(scores)}')\n",
    "# #     print(f'{null} --- {col} : mean f1 score          {np.mean(scores)}')\n",
    "\n",
    "    \n",
    "# #     break\n",
    "# print()\n",
    "# print(f'mean of all scores: {np.mean(scores_all)} ,max: {max(scores_all)}, min: {min(scores_all)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- Age : mean f1 score          0.47541014365398243\n",
      "2 --- Income : mean f1 score          0.4426199079443459\n",
      "47 --- Emp_stat_Present : mean f1 score          0.6087385815775356\n",
      "0 --- Gender : mean f1 score          0.6024368823388777\n",
      "2 --- Marital_status : mean f1 score          0.9331064184946035\n",
      "6 --- Loc : mean f1 score          0.6165197403998646\n",
      "6 --- Notice_things_Present : mean f1 score          0.5862860480489823\n",
      "6 --- All_angles_Present : mean f1 score          0.5891271986924085\n",
      "1 --- Sincere_prod_Present : mean f1 score          0.5472647202465671\n",
      "1 --- Lonely_Present : mean f1 score          0.2913051824453698\n",
      "3 --- Worry_health_Present : mean f1 score          0.4664529183574048\n",
      "5 --- Charity_Present : mean f1 score          0.38947644832275524\n",
      "2 --- New_env_Present : mean f1 score          0.377846015909736\n",
      "4 --- Meeting_ppl_Present : mean f1 score          0.34983344184913384\n",
      "1 --- Hob_interests_Present : mean f1 score          0.38565685290411167\n",
      "5 --- Surveys_Present : mean f1 score          0.3734306008387019\n",
      "0 --- Spent_onli_Present : mean f1 score          0.5328943096036429\n",
      "6 --- WFH_office_Present : mean f1 score          0.4906425503963753\n",
      "3 --- Ethu_Present : mean f1 score          0.449298197599952\n",
      "24 --- Income_Change : mean f1 score          0.5769012642828264\n",
      "3 --- Smoking_hab_Present : mean f1 score          0.7507698194962313\n",
      "0 --- Drinking_hab_Present : mean f1 score          0.7054079215174951\n",
      "3 --- Sleeping_hab_Present : mean f1 score          0.4587235206923011\n",
      "2 --- Healthy_Lifestyle_Present : mean f1 score          0.3350718634512164\n",
      "75 --- Medi_pref_Present : mean f1 score          0.5802846645685411\n",
      "2 --- Pol_interest_Present : mean f1 score          0.38913570776780404\n",
      "0 --- Internet_interest_Present : mean f1 score          0.4354501090226332\n",
      "2 --- Economy_Manag_intrst_Present : mean f1 score          0.45915054094161245\n",
      "2 --- Medicine_intrst_Present : mean f1 score          0.44497470138496575\n",
      "2 --- Religion_intrst_Present : mean f1 score          0.35259924677118865\n",
      "4 --- Save_all_money_Present : mean f1 score          0.4201618325292317\n",
      "1 --- Brand_non-brand_Present : mean f1 score          0.440594826228294\n",
      "16 --- Food_pref_Present : mean f1 score          0.8727015204388172\n",
      "17 --- Mode_of_transport_Present : mean f1 score          0.5179386557869632\n",
      "2 --- Basic_medications_Present : mean f1 score          0.6557495714333275\n",
      "1 --- Digital_content_Present : mean f1 score          0.5464401021851399\n",
      "1 --- Active_Investor_Present : mean f1 score          0.3094143516700926\n",
      "1 --- Edu_instit_fee_Present : mean f1 score          0.44889543477763477\n",
      "15 --- Change_in_data_consumption : mean f1 score          0.5421009123718816\n",
      "3 --- Spend_on_gadgets_Present : mean f1 score          0.36114660534168486\n",
      "4 --- Spend_on_Luxury_Present : mean f1 score          0.48811388190872407\n",
      "4 --- Track_Household_exp_Present : mean f1 score          0.6097272535072287\n",
      "8 --- Domestic_help_Present : mean f1 score          0.3629159852256399\n",
      "5 --- Spending_kind_Present : mean f1 score          0.5428563022832529\n",
      "\n",
      "mean of all scores: 0.502626653527479 ,max: 0.9331064184946035, min: 0.2913051824453698\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "# nulls = data_present_copy.isnull().sum().values\n",
    "# scores_all = []\n",
    "\n",
    "# for col,null in zip(data_present_copy.columns, nulls):\n",
    "\n",
    "#     dt = GradientBoostingClassifier(random_state=0,n_estimators=200,max_depth=4)\n",
    "    \n",
    "#     med_x_not_null,med_y_x_not_null,_,_,_ = sort_data(data_present_copy,col)\n",
    "#     scores=cross_val_score(dt,med_x_not_null,med_y_x_not_null,cv=5,scoring='f1_weighted')\n",
    "    \n",
    "#     scores_all.append(np.mean(scores))\n",
    "#     print(f'{col} : mean f1 score          {np.mean(scores)}')\n",
    "# #     print(f'{null} --- {col} : mean f1 score          {np.mean(scores)}')\n",
    "\n",
    "    \n",
    "# #     break\n",
    "# print()\n",
    "# print(f'mean of all scores: {np.mean(scores_all)} ,max: {max(scores_all)}, min: {min(scores_all)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nulls = data_present_copy.isnull().sum().values\n",
    "index = data_present_copy.isnull().sum().index\n",
    "\n",
    "columns_to_predict = []\n",
    "for index,value in zip(index,nulls):\n",
    "    if value > 0:\n",
    "#         print(index , value)\n",
    "        columns_to_predict.append(index)\n",
    "        \n",
    "scores_all = []\n",
    "\n",
    "for col,null in zip(columns_to_predict, nulls):\n",
    "    \n",
    "    dt = GradientBoostingClassifier(random_state=0,n_estimators=200,max_depth=4)\n",
    "    column_x_not_null,column_y_x_not_null,column_y_predict,column_y_predict_index,column_x_predict = sort_data(data_present_copy,col)\n",
    "    \n",
    "    dt.fit(column_x_not_null,column_y_x_not_null)\n",
    "    \n",
    "    column_x_predict.replace({np.NaN: 0.001},inplace = True)\n",
    "    predicted = dt.predict(column_x_predict)\n",
    "        \n",
    "    for value,index in zip(predicted, column_y_predict_index):\n",
    "        data_present_copy.loc[index,col] = value\n",
    "    \n",
    "#     scores\n",
    "#     scores=cross_val_score(dt,column_x_not_null,column_y_x_not_null,cv=5,scoring='f1_weighted')\n",
    "#     scores_all.append(np.mean(scores))\n",
    "#     print(f'{col} : mean f1 score          {np.mean(scores)}')\n",
    "#     print(f'{null} --- {col} : mean f1 score          {np.mean(scores)}')\n",
    "    \n",
    "    \n",
    "#     break\n",
    "# print()\n",
    "# print(f'mean of all scores: {np.mean(scores_all)} ,max: {max(scores_all)}, min: {min(scores_all)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nulls = data_before_copy.isnull().sum().values\n",
    "index = data_before_copy.isnull().sum().index\n",
    "\n",
    "columns_to_predict = []\n",
    "for index,value in zip(index,nulls):\n",
    "    if value > 0:\n",
    "#         print(index , value)\n",
    "        columns_to_predict.append(index)\n",
    "        \n",
    "scores_all = []\n",
    "\n",
    "for col,null in zip(columns_to_predict, nulls):\n",
    "    \n",
    "    dt = GradientBoostingClassifier(random_state=0,n_estimators=200,max_depth=4)\n",
    "    column_x_not_null,column_y_x_not_null,column_y_predict,column_y_predict_index,column_x_predict = sort_data(data_before_copy,col)\n",
    "    \n",
    "    dt.fit(column_x_not_null,column_y_x_not_null)\n",
    "    \n",
    "    column_x_predict.replace({np.NaN: 0.001},inplace = True)\n",
    "    predicted = dt.predict(column_x_predict)\n",
    "        \n",
    "    for value,index in zip(predicted, column_y_predict_index):\n",
    "        data_before_copy.loc[index,col] = value\n",
    "    \n",
    "#     scores\n",
    "#     scores=cross_val_score(dt,column_x_not_null,column_y_x_not_null,cv=5,scoring='f1_weighted')\n",
    "#     scores_all.append(np.mean(scores))\n",
    "#     print(f'{col} : mean f1 score          {np.mean(scores)}')\n",
    "#     print(f'{null} --- {col} : mean f1 score          {np.mean(scores)}')\n",
    "    \n",
    "    \n",
    "#     break\n",
    "print()\n",
    "# print(f'mean of all scores: {np.mean(scores_all)} ,max: {max(scores_all)}, min: {min(scores_all)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_present_copy.to_csv('Present_Encoded_Imputed_R.csv')\n",
    "data_before_copy.to_csv('Before_Encoded_Imputed_R.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
